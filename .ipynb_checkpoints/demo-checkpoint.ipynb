{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional as tf\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pdb\n",
    "import cv2\n",
    "import random\n",
    "import glob\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from unet_tile_se_norm import UNetTileSENorm\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "# unet_tile_se_inter\n",
    "# from utils import np2tensor\n",
    "\n",
    "class OPT():\n",
    "    pass\n",
    "\n",
    "def np2tensor(numpy_array):\n",
    "    tensor = torch.from_numpy(np.transpose(numpy_array.copy(), (2, 0, 1))).float()/255.*2. - 1\n",
    "    return tensor.unsqueeze(0)\n",
    "\n",
    "def tensor2im(input_image, imtype=np.uint8):\n",
    "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
    "\n",
    "    Parameters:\n",
    "        input_image (tensor) --  the input image tensor array\n",
    "        imtype (type)        --  the desired type of the converted numpy array\n",
    "    \"\"\"\n",
    "    if not isinstance(input_image, np.ndarray):\n",
    "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
    "            image_tensor = input_image.data\n",
    "        else:\n",
    "            return input_image\n",
    "        image_numpy = image_tensor.float().numpy()  # convert it into a numpy array\n",
    "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
    "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
    "        image_numpy = np.maximum(image_numpy, 0)\n",
    "        image_numpy = np.minimum(image_numpy, 255)\n",
    "    else:  # if it is a numpy array, do nothing\n",
    "        image_numpy = input_image\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def get_transform(opt, params=None, grayscale=False, method=Image.BICUBIC, convert=True):\n",
    "    transform_list = []\n",
    "    if grayscale:\n",
    "        transform_list.append(transforms.Grayscale(1))\n",
    "    if 'resize' in opt.preprocess:\n",
    "        osize = [opt.load_size, opt.load_size]\n",
    "        transform_list.append(transforms.Resize(osize, method))\n",
    "    elif 'scale_width' in opt.preprocess:\n",
    "        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.load_size, method)))\n",
    "\n",
    "    if 'crop' in opt.preprocess:\n",
    "        if params is None:\n",
    "            transform_list.append(transforms.RandomCrop(opt.crop_size))\n",
    "        else:\n",
    "            transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt.crop_size)))\n",
    "\n",
    "    if opt.preprocess == 'none':\n",
    "        pass\n",
    "#         transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base=4, method=method)))\n",
    "\n",
    "    if not opt.no_flip:\n",
    "        if params is None:\n",
    "            transform_list.append(transforms.RandomHorizontalFlip())\n",
    "        elif params['flip']:\n",
    "            transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))\n",
    "\n",
    "    if convert:\n",
    "        transform_list += [transforms.ToTensor()]\n",
    "        if grayscale:\n",
    "            transform_list += [transforms.Normalize((0.5,), (0.5,))]\n",
    "        else:\n",
    "            transform_list += [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "*                                                                             *\n",
      "*   Interpreter :                                                             *\n",
      "*       python : 3.6.10 |Anaconda, Inc.| (default, May  7 2020, 23:06:31)     *\n",
      "*                [GCC 4.2.1 Compatible Clang 4.0.1                            *\n",
      "*   (tags/RELEASE_401/final)]                                                 *\n",
      "*                                                                             *\n",
      "*   colour-science.org :                                                      *\n",
      "*       colour : 0.3.15                                                       *\n",
      "*       colour-checker-detection : 0.1.1                                      *\n",
      "*                                                                             *\n",
      "*   Runtime :                                                                 *\n",
      "*       imageio : 2.9.0                                                       *\n",
      "*       matplotlib : 3.3.0                                                    *\n",
      "*       networkx : 2.5                                                        *\n",
      "*       numpy : 1.19.5                                                        *\n",
      "*       scipy : 1.5.1                                                         *\n",
      "*       six : 1.15.0                                                          *\n",
      "*       opencv : 4.4.0                                                        *\n",
      "*                                                                             *\n",
      "===============================================================================\n",
      "216\n",
      "0\n",
      "pexels-johannes-plenio-1123445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yifanjiang/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/functional.py:3509: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/Users/yifanjiang/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "/Users/yifanjiang/anaconda3/envs/torch/lib/python3.6/site-packages/colour/utilities/verbose.py:235: ColourUsageWarning: \"OpenImageIO\" related API features are not available, switching to \"Imageio\"!\n",
      "  warn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-eb2e689925b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    193\u001b[0m             output = colour.cctf_encoding(\n\u001b[1;32m    194\u001b[0m                     colour.colour_correction(\n\u001b[0;32m--> 195\u001b[0;31m                         colour.cctf_decoding(colour.io.read_image(fore_shift_path)), color_0, color_1, terms = 17))\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/colour/characterisation/correction.py\u001b[0m in \u001b[0;36mcolour_correction\u001b[0;34m(RGB, M_T, M_R, method, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0mfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOLOUR_CORRECTION_METHODS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRGB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_R\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfilter_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/colour/characterisation/correction.py\u001b[0m in \u001b[0;36mcolour_correction_Cheung2004\u001b[0;34m(RGB, M_T, M_R, terms)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0mRGB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRGB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m     \u001b[0mRGB_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmented_matrix_Cheung2004\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRGB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0mCCM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolour_correction_matrix_Cheung2004\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_R\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/colour/characterisation/correction.py\u001b[0m in \u001b[0;36maugmented_matrix_Cheung2004\u001b[0;34m(RGB, terms)\u001b[0m\n\u001b[1;32m    165\u001b[0m         return tstack([\n\u001b[1;32m    166\u001b[0m             \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mR\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         ])\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mterms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import pdb\n",
    "import colour\n",
    "from colour.plotting import *\n",
    "import imageio\n",
    "from colour_checker_detection import (\n",
    "    EXAMPLES_RESOURCES_DIRECTORY,\n",
    "    colour_checkers_coordinates_segmentation,\n",
    "    detect_colour_checkers_segmentation)\n",
    "\n",
    "from colour_checker_detection.detection.segmentation import (\n",
    "    adjust_image)\n",
    "\n",
    "colour.utilities.describe_environment();\n",
    "colour_style();\n",
    "\n",
    "opt = OPT()\n",
    "opt.norm = \"IN\"\n",
    "opt.preprocess = \"none\"\n",
    "opt.no_flip = True\n",
    "transform = get_transform(opt)\n",
    "\n",
    "\n",
    "fore_list = glob.glob('./portrait_image/*_cloud_input_WB.jpg') + glob.glob('./portrait2/*_cloud_input_WB.jpg')\n",
    "back_list = glob.glob('./street_image/*.jpg') + glob.glob('./background_image/*.jpg')+ glob.glob('./background2/*.jpg')\n",
    "# back_list = glob.glob('./portrait2/*_cloud_input_WB.jpg') + glob.glob('./portrait_image/*_cloud_input_WB.jpg') + glob.glob('./street_image/*_WB.jpg') + glob.glob('./background_image/*_WB.jpg')\n",
    "\n",
    "def fitting(fore_name, fore_path, dir_path, example_name, mask_0, back_image_0):\n",
    "    EXAMPLES_RESOURCES_DIRECTORY = dir_path\n",
    "    COLOUR_CHECKER_IMAGE_PATHS = glob.glob(\n",
    "        os.path.join(EXAMPLES_RESOURCES_DIRECTORY, '*.jpg'))\n",
    "    COLOUR_CHECKER_IMAGES = [\n",
    "        colour.cctf_decoding(colour.io.read_image(path))\n",
    "        for path in COLOUR_CHECKER_IMAGE_PATHS\n",
    "    ]\n",
    "\n",
    "    for iteration in range(1):\n",
    "\n",
    "        color_0 = cv2.resize(COLOUR_CHECKER_IMAGES[1], (64,64)).reshape((-1,3))\n",
    "        color_1 = cv2.resize(COLOUR_CHECKER_IMAGES[0], (64,64)).reshape((-1,3))\n",
    "        output = colour.cctf_encoding(\n",
    "                colour.colour_correction(\n",
    "                    colour.cctf_decoding(colour.io.read_image(fore_path)), color_0, color_1, terms = 17))\n",
    "        output = np.clip(output, 0, 1)\n",
    "\n",
    "        comp = output*(mask_0) + back_image_0 * (1-mask_0)\n",
    "\n",
    "        imageio.imwrite('./dovenet_compare_psnr_lr/'+ fore_name+example_name,input_image*255)\n",
    "        \n",
    "def process(inp_img, ref_img, inp_show, model_1, dir_path):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    SS_scratch_output, _, _ = model_1(inp_img, ref_img)\n",
    "    SS_scratch_show = tensor2im(SS_scratch_output[0])\n",
    "    SS_scratch_show = SS_scratch_show.astype(float)/255\n",
    "    Image.fromarray(np.uint8(inp_show*255)).save(\"./\" + dir_path + \"/input.jpg\")\n",
    "    Image.fromarray(np.uint8(SS_scratch_show*255)).save(\"./\" + dir_path + \"/result.jpg\")\n",
    "\n",
    "image_small_list = []\n",
    "for root, dirs, files in os.walk(\"../RealHM/vendor_testing_1/\"):\n",
    "    for file in files:\n",
    "        if \"small\" in file:\n",
    "            image_small_list.append(os.path.join(root, file))\n",
    "for root, dirs, files in os.walk(\"../RealHM/vendor_testing_2/\"):\n",
    "    for file in files:\n",
    "        if \"small\" in file:\n",
    "            image_small_list.append(os.path.join(root, file))\n",
    "            \n",
    "for root, dirs, files in os.walk(\"../RealHM/vendor_testing_3/\"):\n",
    "    for file in files:\n",
    "        if \"small\" in file:\n",
    "            image_small_list.append(os.path.join(root, file))\n",
    "            \n",
    "print(len(image_small_list))\n",
    "for index, file in enumerate(image_small_list):\n",
    "    if 1 == 1:\n",
    "        print(index)\n",
    "        fore_path = file\n",
    "        fore_shift_path = file.replace(\"_small.jpg\", \".jpg\")\n",
    "        mask_path = file.replace(\"_small.jpg\", \"_mask.jpg\")\n",
    "        bg_path = file.replace(\"_small.jpg\", \"_fore.jpg\")\n",
    "        gt_path = file.replace(\"_small.jpg\", \"_gt.jpg\")\n",
    "        fore_name = bg_path.replace(\"_fore.jpg\", \"\").replace(\"../vendor_testing_2/\", \"\").replace(\"../vendor_testing_1/\", \"\").replace(\"../vendor_testing_3/\", \"\")\n",
    "        fore_name = fore_name.split(\"/\")[-1]\n",
    "#         if os.path.isfile('./output/'+ fore_name+'_ssh.jpg'):\n",
    "#             continue\n",
    "        inp_img = Image.open(fore_path).convert('RGB')\n",
    "        inp_shift_img = Image.open(fore_shift_path).convert('RGB')\n",
    "        ref_img = Image.open(bg_path).convert('RGB')\n",
    "        mask_img = Image.open(mask_path).convert('RGB')\n",
    "        gt_img = Image.open(gt_path).convert('RGB')\n",
    "\n",
    "        inp_img = tf.resize(inp_img, [256, 256])\n",
    "        inp_shift_img = tf.resize(inp_shift_img, [256, 256])\n",
    "        ref_img = tf.resize(ref_img, [256, 256])\n",
    "        mask_img = tf.resize(mask_img, [256, 256])\n",
    "        gt_img = tf.resize(gt_img, [256, 256])\n",
    "        \n",
    "        inp_img = np.array(inp_img)\n",
    "        inp_shift_img = np.array(inp_shift_img)\n",
    "        ref_img = np.array(ref_img)\n",
    "        mask_img = np.array(mask_img)\n",
    "        gt_img = np.array(gt_img)\n",
    "\n",
    "        inp_img = np2tensor(inp_img)\n",
    "        inp_shift_img = np2tensor(inp_shift_img)\n",
    "        ref_img = np2tensor(ref_img)\n",
    "        mask_img = np2tensor(mask_img)\n",
    "\n",
    "#         ratio = 0.95\n",
    "#         output, _, style = model_inter(inp_img, ref_img, inp_img, ratio)\n",
    "\n",
    "        inp_show = tensor2im(inp_img[0])\n",
    "        inp_shift_show = tensor2im(inp_shift_img[0])\n",
    "        ref_show = tensor2im(ref_img[0])\n",
    "#         oup_show = tensor2im(output[0])\n",
    "\n",
    "        # import pdb\n",
    "        mask_img1 = tensor2im(mask_img.squeeze())\n",
    "        mask_img1 = mask_img1.astype(float)/255\n",
    "        # pdb.set_trace()\n",
    "\n",
    "#         oup_show = oup_show.astype(float)/255\n",
    "        ref_show = ref_show.astype(float)/255\n",
    "        inp_show = inp_show.astype(float)/255\n",
    "        inp_shift_show = inp_shift_show.astype(float)/255\n",
    "\n",
    "#         comp_our = oup_show*(mask_img1)+ref_show*(1-mask_img1)\n",
    "        comp_input = inp_shift_show*(mask_img1)+ref_show*(1-mask_img1)\n",
    "\n",
    "        mask_img1 = mask_img1[:,:,0]\n",
    "        comp = Image.fromarray(np.uint8(comp_input*255))\n",
    "        mask = Image.fromarray(np.uint8(mask_img1*255))\n",
    "        real = Image.fromarray(np.uint8(comp_input*255))\n",
    "\n",
    "        # apply the same transform to composite and real images\n",
    "        comp = transform(comp)\n",
    "        mask = tf.to_tensor(mask)\n",
    "        real = transform(real)\n",
    "        # concate the composite and mask as the input of generator\n",
    "        inputs=torch.cat([comp,mask],0)\n",
    "\n",
    "#         dove_output = DoveNet_model(inputs.unsqueeze(0))\n",
    "        mask = mask.unsqueeze(0)\n",
    "        input_show = tensor2im(comp)\n",
    "#         output_show = tensor2im(dove_output[0])\n",
    "\n",
    "        mask_img1_3 = np.zeros_like(comp_input)\n",
    "        mask_img1_3[:,:,0] = mask_img1\n",
    "        mask_img1_3[:,:,1] = mask_img1\n",
    "        mask_img1_3[:,:,2] = mask_img1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        input_image = np.array(Image.open(fore_shift_path).convert('RGB')).astype(float)/255\n",
    "        back_image = np.array(Image.open(bg_path).convert('RGB')).astype(float)/255\n",
    "        mask = np.array(Image.open(mask_path).convert('RGB')).astype(float)\n",
    "\n",
    "        mask3 = np.zeros_like(input_image)\n",
    "        mask3[:,:,0] = mask[:,:,0]/255\n",
    "        mask3[:,:,1] = mask[:,:,0]/255\n",
    "        mask3[:,:,2] = mask[:,:,0]/255  \n",
    "        print(fore_name)         \n",
    "            \n",
    "        checkpoint = torch.load(\"./checkpoint\", map_location='cpu')\n",
    "        opt.norm = \"BN\"\n",
    "        opt.style_norm = \"BN\"\n",
    "        model_1 = UNetTileSENorm(opt)\n",
    "        model_1.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model_1.eval()\n",
    "        process(inp_img, ref_img, inp_show, model_1, \"NEW_SS_transform_2\")\n",
    "#             fitting(fore_name, fore_path, 'NEW_SS_transform_1', '_64_128_cycle0.1_newcube.jpg', mask3, back_image)\n",
    "        EXAMPLES_RESOURCES_DIRECTORY = 'NEW_SS_transform_2'\n",
    "        COLOUR_CHECKER_IMAGE_PATHS = glob.glob(os.path.join(EXAMPLES_RESOURCES_DIRECTORY, '*.jpg'))\n",
    "        COLOUR_CHECKER_IMAGES = [\n",
    "            colour.cctf_decoding(colour.io.read_image(path))\n",
    "            for path in COLOUR_CHECKER_IMAGE_PATHS\n",
    "        ]\n",
    "\n",
    "        for iteration in range(1):\n",
    "\n",
    "            color_0 = cv2.resize(COLOUR_CHECKER_IMAGES[1], (64,64)).reshape((-1,3))\n",
    "            color_1 = cv2.resize(COLOUR_CHECKER_IMAGES[0], (64,64)).reshape((-1,3))\n",
    "            output = colour.cctf_encoding(\n",
    "                    colour.colour_correction(\n",
    "                        colour.cctf_decoding(colour.io.read_image(fore_shift_path)), color_0, color_1, terms = 17))\n",
    "            output = np.clip(output, 0, 1)\n",
    "\n",
    "            comp = output[:, :, :3]*(mask3) + back_image * (1-mask3)\n",
    "            comp = output[:, :, :3]*(mask3) + back_image * (1-mask3)\n",
    "            comp = comp*255\n",
    "            comp = Image.fromarray(np.uint8(comp))\n",
    "            comp.save('./output/'+ fore_name+'_ssh.jpg')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import compare_mse as mse\n",
    "from skimage.measure import compare_psnr as psnr\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "\n",
    "total_psnr = 0\n",
    "total_ssim = 0\n",
    "total_mse = 0\n",
    "total_lpips = 0\n",
    "index_1 = 0\n",
    "for files, dirs, root in os.walk(\"./output_1\"):\n",
    "    for file in root:\n",
    "        if \".jpg\" in file:\n",
    "#             print(file)\n",
    "            ssh = cv2.imread(\"./output_1/\"+file)\n",
    "            for name in image_small_list:\n",
    "                if file.replace(\"_ssh.jpg\", \"\") in name:\n",
    "#                     print(name)\n",
    "                    index_1 += 1\n",
    "                    gt = cv2.imread(name.replace(\"_small_fore\", \"_small\").replace(\"_small\", \"_gt\"))\n",
    "                    gt = cv2.resize(gt, (256,256), interpolation=cv2.INTER_AREA)\n",
    "                    ssh = cv2.resize(ssh, (256,256), interpolation=cv2.INTER_AREA)\n",
    "                    psnr_1 = psnr(gt, ssh, data_range=ssh.max() - ssh.min())\n",
    "                    \n",
    "                    total_psnr += psnr_1\n",
    "# print(len(image_small_list))\n",
    "print(index_1)\n",
    "print(total_psnr/index_1)\n",
    "                    \n",
    "#             img = cv2.imread(f\"../camera_ready/vendor_testing_3/{file}\")\n",
    "#             cv2.imwrite(f\"../camera_ready/vendor_testing_3/{file}\", img)\n",
    "#             replace = file.replace(\".png\", \".jpg\")\n",
    "#             os.system(f\"mv ../camera_ready/vendor_testing_3/{file} ../camera_ready/vendor_testing_3/{replace}\")\n",
    "#             print(f\"mv ../camera_ready/vendor_testing_3/{file} ../camera_ready/vendor_testing_3/{replace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_1\n",
    "total_psnr/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
