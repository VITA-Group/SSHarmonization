{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional as tf\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pdb\n",
    "import cv2\n",
    "import random\n",
    "import glob\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from unet_tile_se_norm import UNetTileSENorm\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "# unet_tile_se_inter\n",
    "# from utils import np2tensor\n",
    "\n",
    "class OPT():\n",
    "    pass\n",
    "\n",
    "def np2tensor(numpy_array):\n",
    "    tensor = torch.from_numpy(np.transpose(numpy_array.copy(), (2, 0, 1))).float()/255.*2. - 1\n",
    "    return tensor.unsqueeze(0)\n",
    "\n",
    "def tensor2im(input_image, imtype=np.uint8):\n",
    "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
    "\n",
    "    Parameters:\n",
    "        input_image (tensor) --  the input image tensor array\n",
    "        imtype (type)        --  the desired type of the converted numpy array\n",
    "    \"\"\"\n",
    "    if not isinstance(input_image, np.ndarray):\n",
    "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
    "            image_tensor = input_image.data\n",
    "        else:\n",
    "            return input_image\n",
    "        image_numpy = image_tensor.float().numpy()  # convert it into a numpy array\n",
    "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
    "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
    "        image_numpy = np.maximum(image_numpy, 0)\n",
    "        image_numpy = np.minimum(image_numpy, 255)\n",
    "    else:  # if it is a numpy array, do nothing\n",
    "        image_numpy = input_image\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def get_transform(opt, params=None, grayscale=False, method=Image.BICUBIC, convert=True):\n",
    "    transform_list = []\n",
    "    if grayscale:\n",
    "        transform_list.append(transforms.Grayscale(1))\n",
    "    if 'resize' in opt.preprocess:\n",
    "        osize = [opt.load_size, opt.load_size]\n",
    "        transform_list.append(transforms.Resize(osize, method))\n",
    "    elif 'scale_width' in opt.preprocess:\n",
    "        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.load_size, method)))\n",
    "\n",
    "    if 'crop' in opt.preprocess:\n",
    "        if params is None:\n",
    "            transform_list.append(transforms.RandomCrop(opt.crop_size))\n",
    "        else:\n",
    "            transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt.crop_size)))\n",
    "\n",
    "    if opt.preprocess == 'none':\n",
    "        pass\n",
    "#         transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base=4, method=method)))\n",
    "\n",
    "    if not opt.no_flip:\n",
    "        if params is None:\n",
    "            transform_list.append(transforms.RandomHorizontalFlip())\n",
    "        elif params['flip']:\n",
    "            transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))\n",
    "\n",
    "    if convert:\n",
    "        transform_list += [transforms.ToTensor()]\n",
    "        if grayscale:\n",
    "            transform_list += [transforms.Normalize((0.5,), (0.5,))]\n",
    "        else:\n",
    "            transform_list += [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "*                                                                             *\n",
      "*   Interpreter :                                                             *\n",
      "*       python : 3.6.10 |Anaconda, Inc.| (default, May  7 2020, 23:06:31)     *\n",
      "*                [GCC 4.2.1 Compatible Clang 4.0.1                            *\n",
      "*   (tags/RELEASE_401/final)]                                                 *\n",
      "*                                                                             *\n",
      "*   colour-science.org :                                                      *\n",
      "*       colour : 0.3.15                                                       *\n",
      "*       colour-checker-detection : 0.1.1                                      *\n",
      "*                                                                             *\n",
      "*   Runtime :                                                                 *\n",
      "*       imageio : 2.9.0                                                       *\n",
      "*       matplotlib : 3.3.0                                                    *\n",
      "*       networkx : 2.5                                                        *\n",
      "*       numpy : 1.19.5                                                        *\n",
      "*       scipy : 1.5.1                                                         *\n",
      "*       six : 1.15.0                                                          *\n",
      "*       opencv : 4.4.0                                                        *\n",
      "*                                                                             *\n",
      "===============================================================================\n",
      "220\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "forest-931706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yifanjiang/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/functional.py:3328: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/Users/yifanjiang/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "/Users/yifanjiang/anaconda3/envs/torch/lib/python3.6/site-packages/colour/utilities/verbose.py:235: ColourUsageWarning: \"OpenImageIO\" related API features are not available, switching to \"Imageio\"!\n",
      "  warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "desert-2576929_gt920\n",
      "202\n",
      "condominium-690086\n",
      "203\n",
      "winter-2080070\n",
      "204\n",
      "beach-768421\n",
      "205\n",
      "city-1150026\n",
      "206\n",
      "forest-3394066\n",
      "207\n",
      "mountains-440520\n",
      "208\n",
      "seascape-4636264\n",
      "209\n",
      "vaulted-cellar-247391\n",
      "210\n",
      "ameland-5651866\n",
      "211\n",
      "downtown-690827\n",
      "212\n",
      "beach-1236581\n",
      "213\n",
      "alley-3453557\n",
      "214\n",
      "room-597166\n",
      "215\n",
      "benches-560435\n",
      "216\n",
      "neighborhood-802074\n",
      "217\n",
      "beach-5622187\n",
      "218\n",
      "ocean-931776\n",
      "219\n",
      "arches-1850730\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import pdb\n",
    "import colour\n",
    "from colour.plotting import *\n",
    "import imageio\n",
    "from colour_checker_detection import (\n",
    "    EXAMPLES_RESOURCES_DIRECTORY,\n",
    "    colour_checkers_coordinates_segmentation,\n",
    "    detect_colour_checkers_segmentation)\n",
    "\n",
    "from colour_checker_detection.detection.segmentation import (\n",
    "    adjust_image)\n",
    "\n",
    "colour.utilities.describe_environment();\n",
    "colour_style();\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fore_list = glob.glob('./portrait_image/*_cloud_input_WB.jpg') + glob.glob('./portrait2/*_cloud_input_WB.jpg')\n",
    "back_list = glob.glob('./street_image/*.jpg') + glob.glob('./background_image/*.jpg')+ glob.glob('./background2/*.jpg')\n",
    "# back_list = glob.glob('./portrait2/*_cloud_input_WB.jpg') + glob.glob('./portrait_image/*_cloud_input_WB.jpg') + glob.glob('./street_image/*_WB.jpg') + glob.glob('./background_image/*_WB.jpg')\n",
    "\n",
    "def fitting(fore_name, fore_path, dir_path, example_name, mask_0, back_image_0):\n",
    "    EXAMPLES_RESOURCES_DIRECTORY = dir_path\n",
    "    COLOUR_CHECKER_IMAGE_PATHS = glob.glob(\n",
    "        os.path.join(EXAMPLES_RESOURCES_DIRECTORY, '*.jpg'))\n",
    "    COLOUR_CHECKER_IMAGES = [\n",
    "        colour.cctf_decoding(colour.io.read_image(path))\n",
    "        for path in COLOUR_CHECKER_IMAGE_PATHS\n",
    "    ]\n",
    "\n",
    "    for iteration in range(1):\n",
    "\n",
    "        color_0 = cv2.resize(COLOUR_CHECKER_IMAGES[1], (64,64)).reshape((-1,3))\n",
    "        color_1 = cv2.resize(COLOUR_CHECKER_IMAGES[0], (64,64)).reshape((-1,3))\n",
    "        output = colour.cctf_encoding(\n",
    "                colour.colour_correction(\n",
    "                    colour.cctf_decoding(colour.io.read_image(fore_path)), color_0, color_1, terms = 17))\n",
    "        output = np.clip(output, 0, 1)\n",
    "\n",
    "        comp = output*(mask_0) + back_image_0 * (1-mask_0)\n",
    "\n",
    "        imageio.imwrite('./dovenet_compare_psnr_lr/'+ fore_name+example_name,input_image*255)\n",
    "        \n",
    "def process(inp_img, ref_img, inp_show, model_1, dir_path):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    SS_scratch_output, _, _ = model_1(inp_img, ref_img)\n",
    "    SS_scratch_show = tensor2im(SS_scratch_output[0])\n",
    "    SS_scratch_show = SS_scratch_show.astype(float)/255\n",
    "    Image.fromarray(np.uint8(inp_show*255)).save(\"./\" + dir_path + \"/input.jpg\")\n",
    "    Image.fromarray(np.uint8(SS_scratch_show*255)).save(\"./\" + dir_path + \"/result.jpg\")\n",
    "\n",
    "image_small_list = []\n",
    "# for root, dirs, files in os.walk(\"../16_fore_mask/\"):\n",
    "#     for file in files:\n",
    "#         if \"small\" in file:\n",
    "#             image_small_list.append(os.path.join(root, file))\n",
    "for root, dirs, files in os.walk(\"../camera_ready/vendor_testing_1/\"):\n",
    "    for file in files:\n",
    "        if \"small\" in file:\n",
    "            image_small_list.append(os.path.join(root, file))\n",
    "for root, dirs, files in os.walk(\"../camera_ready/vendor_testing_2/\"):\n",
    "    for file in files:\n",
    "        if \"small\" in file:\n",
    "            image_small_list.append(os.path.join(root, file))\n",
    "            \n",
    "for root, dirs, files in os.walk(\"../camera_ready/vendor_testing_3/\"):\n",
    "    for file in files:\n",
    "        if \"small\" in file:\n",
    "            image_small_list.append(os.path.join(root, file))\n",
    "            \n",
    "print(len(image_small_list))\n",
    "for index, file in enumerate(image_small_list):\n",
    "    if 1 == 1:\n",
    "        print(index)\n",
    "        fore_path = file\n",
    "        fore_shift_path = file.replace(\"_small.jpg\", \".jpg\")\n",
    "        mask_path = file.replace(\"_small.jpg\", \"_mask.jpg\")\n",
    "        bg_path = file.replace(\"_small.jpg\", \"_fore.jpg\")\n",
    "        gt_path = file.replace(\"_small.jpg\", \"_gt.jpg\")\n",
    "        fore_name = bg_path.replace(\"_fore.jpg\", \"\").replace(\"../vendor_testing_2/\", \"\").replace(\"../vendor_testing_1/\", \"\").replace(\"../vendor_testing_3/\", \"\")\n",
    "        fore_name = fore_name.split(\"/\")[-1]\n",
    "        if os.path.isfile('./output_1/'+ fore_name+'_ssh.jpg'):\n",
    "            continue\n",
    "#         fore_name = fore_path.split('/')[-1][:-4]\n",
    "#         bg_name = bg_path.split('/')[-1][:-4]\n",
    "#         high_res_path = './final_result/' + fore_name + '/' + bg_name + '.jpg'\n",
    "        inp_img = Image.open(fore_path).convert('RGB')\n",
    "        inp_shift_img = Image.open(fore_shift_path).convert('RGB')\n",
    "        ref_img = Image.open(bg_path).convert('RGB')\n",
    "        mask_img = Image.open(mask_path).convert('RGB')\n",
    "        gt_img = Image.open(gt_path).convert('RGB')\n",
    "\n",
    "        inp_img = tf.resize(inp_img, [256, 256])\n",
    "        inp_shift_img = tf.resize(inp_shift_img, [256, 256])\n",
    "        ref_img = tf.resize(ref_img, [256, 256])\n",
    "        mask_img = tf.resize(mask_img, [256, 256])\n",
    "        gt_img = tf.resize(gt_img, [256, 256])\n",
    "        \n",
    "        inp_img = np.array(inp_img)\n",
    "        inp_shift_img = np.array(inp_shift_img)\n",
    "        ref_img = np.array(ref_img)\n",
    "        mask_img = np.array(mask_img)\n",
    "        gt_img = np.array(gt_img)\n",
    "\n",
    "        inp_img = np2tensor(inp_img)\n",
    "        inp_shift_img = np2tensor(inp_shift_img)\n",
    "        ref_img = np2tensor(ref_img)\n",
    "        mask_img = np2tensor(mask_img)\n",
    "\n",
    "#         ratio = 0.95\n",
    "#         output, _, style = model_inter(inp_img, ref_img, inp_img, ratio)\n",
    "\n",
    "        inp_show = tensor2im(inp_img[0])\n",
    "        inp_shift_show = tensor2im(inp_shift_img[0])\n",
    "        ref_show = tensor2im(ref_img[0])\n",
    "#         oup_show = tensor2im(output[0])\n",
    "\n",
    "        # import pdb\n",
    "        mask_img1 = tensor2im(mask_img.squeeze())\n",
    "        mask_img1 = mask_img1.astype(float)/255\n",
    "        # pdb.set_trace()\n",
    "\n",
    "#         oup_show = oup_show.astype(float)/255\n",
    "        ref_show = ref_show.astype(float)/255\n",
    "        inp_show = inp_show.astype(float)/255\n",
    "        inp_shift_show = inp_shift_show.astype(float)/255\n",
    "\n",
    "#         comp_our = oup_show*(mask_img1)+ref_show*(1-mask_img1)\n",
    "        comp_input = inp_shift_show*(mask_img1)+ref_show*(1-mask_img1)\n",
    "\n",
    "        mask_img1 = mask_img1[:,:,0]\n",
    "        comp = Image.fromarray(np.uint8(comp_input*255))\n",
    "        mask = Image.fromarray(np.uint8(mask_img1*255))\n",
    "        real = Image.fromarray(np.uint8(comp_input*255))\n",
    "\n",
    "        # apply the same transform to composite and real images\n",
    "        comp = transform(comp)\n",
    "        mask = tf.to_tensor(mask)\n",
    "        real = transform(real)\n",
    "        # concate the composite and mask as the input of generator\n",
    "        inputs=torch.cat([comp,mask],0)\n",
    "\n",
    "#         dove_output = DoveNet_model(inputs.unsqueeze(0))\n",
    "        mask = mask.unsqueeze(0)\n",
    "        input_show = tensor2im(comp)\n",
    "#         output_show = tensor2im(dove_output[0])\n",
    "\n",
    "        mask_img1_3 = np.zeros_like(comp_input)\n",
    "        mask_img1_3[:,:,0] = mask_img1\n",
    "        mask_img1_3[:,:,1] = mask_img1\n",
    "        mask_img1_3[:,:,2] = mask_img1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        input_image = np.array(Image.open(fore_shift_path).convert('RGB')).astype(float)/255\n",
    "        back_image = np.array(Image.open(bg_path).convert('RGB')).astype(float)/255\n",
    "        mask = np.array(Image.open(mask_path).convert('RGB')).astype(float)\n",
    "\n",
    "        mask3 = np.zeros_like(input_image)\n",
    "        mask3[:,:,0] = mask[:,:,0]/255\n",
    "        mask3[:,:,1] = mask[:,:,0]/255\n",
    "        mask3[:,:,2] = mask[:,:,0]/255  \n",
    "        print(fore_name)         \n",
    "            \n",
    "        checkpoint = torch.load(\"./checkpoint\", map_location='cpu')\n",
    "        opt.norm = \"BN\"\n",
    "        opt.style_norm = \"BN\"\n",
    "        model_1 = UNetTileSENorm(opt)\n",
    "        model_1.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model_1.eval()\n",
    "        process(inp_img, ref_img, inp_show, model_1, \"NEW_SS_transform_2\")\n",
    "#             fitting(fore_name, fore_path, 'NEW_SS_transform_1', '_64_128_cycle0.1_newcube.jpg', mask3, back_image)\n",
    "        EXAMPLES_RESOURCES_DIRECTORY = 'NEW_SS_transform_2'\n",
    "        COLOUR_CHECKER_IMAGE_PATHS = glob.glob(os.path.join(EXAMPLES_RESOURCES_DIRECTORY, '*.jpg'))\n",
    "        COLOUR_CHECKER_IMAGES = [\n",
    "            colour.cctf_decoding(colour.io.read_image(path))\n",
    "            for path in COLOUR_CHECKER_IMAGE_PATHS\n",
    "        ]\n",
    "\n",
    "        for iteration in range(1):\n",
    "\n",
    "            color_0 = cv2.resize(COLOUR_CHECKER_IMAGES[1], (64,64)).reshape((-1,3))\n",
    "            color_1 = cv2.resize(COLOUR_CHECKER_IMAGES[0], (64,64)).reshape((-1,3))\n",
    "            output = colour.cctf_encoding(\n",
    "                    colour.colour_correction(\n",
    "                        colour.cctf_decoding(colour.io.read_image(fore_shift_path)), color_0, color_1, terms = 17))\n",
    "            output = np.clip(output, 0, 1)\n",
    "\n",
    "            comp = output[:, :, :3]*(mask3) + back_image * (1-mask3)\n",
    "            comp = output[:, :, :3]*(mask3) + back_image * (1-mask3)\n",
    "            comp = comp*255\n",
    "            comp = Image.fromarray(np.uint8(comp))\n",
    "            comp.save('./output_1/'+ fore_name+'_ssh.jpg')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yifanjiang/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: DEPRECATED: skimage.measure.compare_psnr has been moved to skimage.metrics.peak_signal_noise_ratio. It will be removed from skimage.measure in version 0.18.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n",
      "28.0443528771\n"
     ]
    }
   ],
   "source": [
    "from skimage.measure import compare_mse as mse\n",
    "from skimage.measure import compare_psnr as psnr\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "\n",
    "total_psnr = 0\n",
    "total_ssim = 0\n",
    "total_mse = 0\n",
    "total_lpips = 0\n",
    "index_1 = 0\n",
    "for files, dirs, root in os.walk(\"./output_1\"):\n",
    "    for file in root:\n",
    "        if \".jpg\" in file:\n",
    "#             print(file)\n",
    "            ssh = cv2.imread(\"./output_1/\"+file)\n",
    "            for name in image_small_list:\n",
    "                if file.replace(\"_ssh.jpg\", \"\") in name:\n",
    "#                     print(name)\n",
    "                    index_1 += 1\n",
    "                    gt = cv2.imread(name.replace(\"_small_fore\", \"_small\").replace(\"_small\", \"_gt\"))\n",
    "                    gt = cv2.resize(gt, (256,256), interpolation=cv2.INTER_AREA)\n",
    "                    ssh = cv2.resize(ssh, (256,256), interpolation=cv2.INTER_AREA)\n",
    "                    psnr_1 = psnr(gt, ssh, data_range=ssh.max() - ssh.min())\n",
    "                    \n",
    "                    total_psnr += psnr_1\n",
    "# print(len(image_small_list))\n",
    "print(index_1)\n",
    "print(total_psnr/index_1)\n",
    "                    \n",
    "#             img = cv2.imread(f\"../camera_ready/vendor_testing_3/{file}\")\n",
    "#             cv2.imwrite(f\"../camera_ready/vendor_testing_3/{file}\", img)\n",
    "#             replace = file.replace(\".png\", \".jpg\")\n",
    "#             os.system(f\"mv ../camera_ready/vendor_testing_3/{file} ../camera_ready/vendor_testing_3/{replace}\")\n",
    "#             print(f\"mv ../camera_ready/vendor_testing_3/{file} ../camera_ready/vendor_testing_3/{replace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-be92deebb4f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtotal_psnr\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index_1' is not defined"
     ]
    }
   ],
   "source": [
    "index_1\n",
    "total_psnr/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
